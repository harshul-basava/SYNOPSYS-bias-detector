{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# sources that helped us make our model:\n",
        "# 1.https://lajavaness.medium.com/multiclass-and-multilabel-text-classification-in-one-bert-model-95c54aab59dc\n",
        "# helped us figure out a method to do multilabel - multiclass classification while using hugging face\n",
        "# especially helpful with regards to creating a custom loss function\n",
        "# 2. https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "# taught us general information regarding hugging face and how to fine tune our own model using an LLM like BERT"
      ],
      "metadata": {
        "id": "p9OIEzKb7I6B"
      },
      "id": "p9OIEzKb7I6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRyefnQKSpqq",
        "outputId": "0efe50c2-a845-4354-84b8-d893aba7f0de"
      },
      "id": "yRyefnQKSpqq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111112db",
      "metadata": {
        "id": "111112db"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skmultilearn.model_selection.iterative_stratification import iterative_train_test_split as ITTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45174e80",
      "metadata": {
        "id": "45174e80"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "finalData = [[\"Text\", \"Palestine Sympathy Rating\", \"Israel Sympathy Rating\", \"Palestine Military Rating\", \"Israel Military Rating\"]]\n",
        "X = []\n",
        "y = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e264493",
      "metadata": {
        "id": "2e264493"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/finalAllData.csv\", newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_LABELS = {\n",
        "\"PS0\": \"negtive palestine sympathy\",\n",
        "\"PS1\": \"neutral palestine sympathy\",\n",
        "\"PS2\": \"positive palestine sympathy\",\n",
        "\"IS0\": \"negtive israel  sympathy\",\n",
        "\"IS1\": \"neutral israel  sympathy\",\n",
        "\"IS2\": \"positive israel sympathy\",\n",
        "\"PM0\": \"negative palestine military\",\n",
        "\"PM1\": \"neutral palestine military\",\n",
        "\"PM2\": \"positive palestine military\",\n",
        "\"IM0\": \"negative israel military\",\n",
        "\"IM1\": \"neutral israel military\",\n",
        "\"IM2\": \"positive israel military\"\n",
        "}"
      ],
      "metadata": {
        "id": "FB_d7qdNbx8j"
      },
      "id": "FB_d7qdNbx8j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2e38cc",
      "metadata": {
        "id": "9e2e38cc"
      },
      "outputs": [],
      "source": [
        "def redefine(ls):\n",
        "    #-1, 0, and 1\n",
        "    for i in range(4):\n",
        "        if ls[i].strip() == \"+\":\n",
        "            ls[i] = 1.0\n",
        "        elif ls[i].strip() == \"-\":\n",
        "            ls[i] = -1.0\n",
        "        elif ls[i].lower().strip() == \"n\":\n",
        "            ls[i] = 0.0\n",
        "\n",
        "    return ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4350319",
      "metadata": {
        "id": "f4350319"
      },
      "outputs": [],
      "source": [
        "for z in range(1, len(data)):\n",
        "    x = data[z]\n",
        "    X.append([x[0]])\n",
        "\n",
        "    ls = x[1:]\n",
        "    ls = redefine(ls)\n",
        "    y.append(ls)\n",
        "    _ = []\n",
        "    _.append(x[0])\n",
        "    _.append(ls)\n",
        "\n",
        "    finalData.append(_)\n",
        "\n",
        "print(len(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a9e72b",
      "metadata": {
        "id": "e7a9e72b"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/test_data.csv\", newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    testData = list(reader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "\n",
        "for z in range(1, len(testData)):\n",
        "    x = testData[z]\n",
        "    X_test.append([x[0]])\n",
        "\n",
        "    ls = x[1:]\n",
        "    ls = redefine(ls)\n",
        "    y_test.append(ls)"
      ],
      "metadata": {
        "id": "9Oi8efQanzxp"
      },
      "id": "9Oi8efQanzxp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3864e9cb",
      "metadata": {
        "id": "3864e9cb"
      },
      "outputs": [],
      "source": [
        "def ratingDisplay(providedList):\n",
        "    ratingsDict = {}\n",
        "    for ls in providedList:\n",
        "        if str(ls) in ratingsDict:\n",
        "            ratingsDict[str(ls)] += 1\n",
        "        else:\n",
        "            ratingsDict[str(ls)] = 1\n",
        "\n",
        "    for key in ratingsDict:\n",
        "        print(key + \": \" + str(ratingsDict[key]))\n",
        "    print(sum(ratingsDict.values()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deleted = []"
      ],
      "metadata": {
        "id": "fHv5dElRqte4"
      },
      "id": "fHv5dElRqte4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[15] = ['\"There is no valid reason to believe that Palestinian claim to the land is \"finished\". The Palestinian expectation has never been that the Israelis give up \"everything\". The expectation has been to stop the colonization, and allow Palestinian self-rule over their land.\"']\n",
        "X_test[33] = ['\"The problem with that argument is that depending on which poll Hamas already has between 60% and 80% of open and enthusiastic support of Palestinians. People love to make excuses for the Palestinian Population not being Hamas, but the fact is most share their worldview and are willing to follow Hamas and fight, rape, torture and murder \"the jews\".\"']\n",
        "X_test[34] = ['''\"The videos of 7th October that Hamas launched shows Palestinian civilians including kids taking Israeli women as hostages. Why didn't Palestinians stick to the two state solution when Arafat signed the peace treaty? And why do Palestinians claim it as their land when everyone knows that it's an ancestral jew land? Remember, it wasn't Hamas with all those suicide bombings in Israel all throughout the creation of \"new Israel\", it was Palestinian civilians.\"''']\n",
        "X_test[55] = ['''\"The premise of Israel's legitimacy doesn't make sense. You call another people's land halfway across the world which you know nothing your \"ancestral homeland\". You play the \"historical persecution\" to justify your existence. You terrorize and genocide the local indigenous population and try to push then out. You rely of systematic oppression and murder for your security. You take over their culture and rename it in your language, which is actually your second.\"''']\n",
        "X_test[65] = ['\"You portray Palestinians as being too proud to \"surrender to peace.\" Surrendering to a life of indignity under Israeli blockade, where basic human needs and rights are denied and there is no legal protection from the excesses of the occupation. Are you expecting them to silently accept the building of Israeli settlements over their land.\"']\n",
        "\n",
        "for x in X_test:\n",
        "    bob = True\n",
        "    w = x[0]\n",
        "    w = w[1:-1]\n",
        "    for z in X:\n",
        "        if w in z[0]:\n",
        "            index = X.index(z)\n",
        "            del X[index]\n",
        "            del y[index]\n",
        "            bob = False\n",
        "            deleted.append(x)\n",
        "    if bob and x not in deleted:\n",
        "        print(X_test.index(x))\n",
        "        print(x[0])\n",
        "\n",
        "print(len(X))"
      ],
      "metadata": {
        "id": "mfWWJuzFndSE"
      },
      "id": "mfWWJuzFndSE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_sympathy_p = []\n",
        "y_sympathy_i = []\n",
        "y_military_p = []\n",
        "y_military_i = []\n",
        "\n",
        "for dataPoint in y:\n",
        "  y_sympathy_p.append(dataPoint[0])\n",
        "  y_sympathy_i.append(dataPoint[1])\n",
        "  y_military_p.append(dataPoint[2])\n",
        "  y_military_i.append(dataPoint[3])\n",
        "\n",
        "ratingDisplay(y_sympathy_p)\n",
        "ratingDisplay(y_sympathy_i)\n",
        "ratingDisplay(y_military_p)\n",
        "ratingDisplay(y_military_i)"
      ],
      "metadata": {
        "id": "qdJ_muPwevBt"
      },
      "id": "qdJ_muPwevBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test).astype(float)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y).astype(float)"
      ],
      "metadata": {
        "id": "xVrpVpyPP-Fv"
      },
      "id": "xVrpVpyPP-Fv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_validation, y_validation = ITTS(X, y, test_size=0.125)\n",
        "\n",
        "X_validation = np.array(X_validation)\n",
        "y_validation = np.array(y_validation).astype(float)"
      ],
      "metadata": {
        "id": "o5hXmUKQYim8"
      },
      "id": "o5hXmUKQYim8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c12e7460",
      "metadata": {
        "id": "c12e7460"
      },
      "outputs": [],
      "source": [
        "correlations = np.corrcoef(y.T)\n",
        "correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993b8241",
      "metadata": {
        "scrolled": false,
        "id": "993b8241"
      },
      "outputs": [],
      "source": [
        "# Palestine Sympathy--Israel Sympathy--Palestine Military--Israel Military\n",
        "#create a new figure\n",
        "cols = [\"PSympathy\", \"ISympathy\", \"PMilitary\", \"IMilitary\"]\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "#111: 1x1 grid, first subplot\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "#normalize data using vmin, vmax\n",
        "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
        "\n",
        "#add a colorbar to a plot.\n",
        "fig.colorbar(cax)\n",
        "\n",
        "#define ticks\n",
        "ticks = np.arange(0,4)\n",
        "\n",
        "#set x and y tick marks\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_yticks(ticks)\n",
        "\n",
        "#set x and y tick labels\n",
        "ax.set_xticklabels(cols)\n",
        "ax.set_yticklabels(cols)\n",
        "\n",
        "#draw a matrix using the correlations data\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea92f0f1",
      "metadata": {
        "scrolled": false,
        "id": "ea92f0f1"
      },
      "outputs": [],
      "source": [
        "print(\"train set:\")\n",
        "# ratingDisplay(y_train)\n",
        "\n",
        "y_sympathy_p_train = []\n",
        "y_sympathy_i_train = []\n",
        "y_military_p_train = []\n",
        "y_military_i_train = []\n",
        "\n",
        "for dataPoint in y_train:\n",
        "  if dataPoint[0] == -1.:\n",
        "    y_sympathy_p_train.append(\"PS0\")\n",
        "  elif dataPoint[0] == 0.:\n",
        "    y_sympathy_p_train.append(\"PS1\")\n",
        "  else:\n",
        "    y_sympathy_p_train.append(\"PS2\")\n",
        "  if dataPoint[1] == -1.:\n",
        "    y_sympathy_i_train.append(\"IS0\")\n",
        "  elif dataPoint[1] == 0.:\n",
        "    y_sympathy_i_train.append(\"IS1\")\n",
        "  else:\n",
        "    y_sympathy_i_train.append(\"IS2\")\n",
        "  if dataPoint[2] == -1.:\n",
        "    y_military_p_train.append(\"PM0\")\n",
        "  elif dataPoint[2] == 0.:\n",
        "    y_military_p_train.append(\"PM1\")\n",
        "  else:\n",
        "    y_military_p_train.append(\"PM2\")\n",
        "  if dataPoint[3] == -1.:\n",
        "    y_military_i_train.append(\"IM0\")\n",
        "  elif dataPoint[3] == 0.:\n",
        "    y_military_i_train.append(\"IM1\")\n",
        "  else:\n",
        "    y_military_i_train.append(\"IM2\")\n",
        "\n",
        "print(\"\\nvalidation set:\")\n",
        "# ratingDisplay(y_validation)\n",
        "\n",
        "y_sympathy_p_val = []\n",
        "y_sympathy_i_val = []\n",
        "y_military_p_val = []\n",
        "y_military_i_val = []\n",
        "\n",
        "for dataPoint in y_validation:\n",
        "  if dataPoint[0] == -1.:\n",
        "    y_sympathy_p_val.append(\"PS0\")\n",
        "  elif dataPoint[0] == 0.:\n",
        "    y_sympathy_p_val.append(\"PS1\")\n",
        "  else:\n",
        "    y_sympathy_p_val.append(\"PS2\")\n",
        "  if dataPoint[1] == -1.:\n",
        "    y_sympathy_i_val.append(\"IS0\")\n",
        "  elif dataPoint[1] == 0.:\n",
        "    y_sympathy_i_val.append(\"IS1\")\n",
        "  else:\n",
        "    y_sympathy_i_val.append(\"IS2\")\n",
        "  if dataPoint[2] == -1.:\n",
        "    y_military_p_val.append(\"PM0\")\n",
        "  elif dataPoint[2] == 0.:\n",
        "    y_military_p_val.append(\"PM1\")\n",
        "  else:\n",
        "    y_military_p_val.append(\"PM2\")\n",
        "  if dataPoint[3] == -1.:\n",
        "    y_military_i_val.append(\"IM0\")\n",
        "  elif dataPoint[3] == 0.:\n",
        "    y_military_i_val.append(\"IM1\")\n",
        "  else:\n",
        "    y_military_i_val.append(\"IM2\")\n",
        "\n",
        "\n",
        "print(\"\\ntest set:\")\n",
        "# ratingDisplay(y_test)\n",
        "\n",
        "y_sympathy_p_test = []\n",
        "y_sympathy_i_test = []\n",
        "y_military_p_test = []\n",
        "y_military_i_test = []\n",
        "\n",
        "for dataPoint in y_test:\n",
        "  if dataPoint[0] == -1.:\n",
        "    y_sympathy_p_test.append(\"PS0\")\n",
        "  elif dataPoint[0] == 0.:\n",
        "    y_sympathy_p_test.append(\"PS1\")\n",
        "  else:\n",
        "    y_sympathy_p_test.append(\"PS2\")\n",
        "  if dataPoint[1] == -1.:\n",
        "    y_sympathy_i_test.append(\"IS0\")\n",
        "  elif dataPoint[1] ==0.:\n",
        "    y_sympathy_i_test.append(\"IS1\")\n",
        "  else:\n",
        "    y_sympathy_i_test.append(\"IS2\")\n",
        "  if dataPoint[2] == -1.:\n",
        "    y_military_p_test.append(\"PM0\")\n",
        "  elif dataPoint[2] == 0.:\n",
        "    y_military_p_test.append(\"PM1\")\n",
        "  else:\n",
        "    y_military_p_test.append(\"PM2\")\n",
        "  if dataPoint[3] == -1.:\n",
        "    y_military_i_test.append(\"IM0\")\n",
        "  elif dataPoint[3] == 0.:\n",
        "    y_military_i_test.append(\"IM1\")\n",
        "  else:\n",
        "    y_military_i_test.append(\"IM2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "CHECKPOINT = \"bert-base-uncased\"\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 6\n",
        "\n",
        "id2label = {k:v for k, v in enumerate(ALL_LABELS)}\n",
        "label2id = {v:k for k, v in enumerate(ALL_LABELS)}"
      ],
      "metadata": {
        "id": "dKxY0g4cZFi6"
      },
      "id": "dKxY0g4cZFi6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d44ffb",
      "metadata": {
        "id": "b0d44ffb"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT, id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# credit: https://lajavaness.medium.com/multiclass-and-multilabel-text-classification-in-one-bert-model-95c54aab59dc\n",
        "# function for turning our old labels (4 long) to our new array of labels (12 long) that our model can use\n",
        "def preprocess_function(examples: dict):\n",
        "    labels = [0] * len(id2label)\n",
        "    for k, l in id2label.items():\n",
        "        if l == examples[\"palestine_sympathy\"] or l == examples[\"israel_sympathy\"] or l == examples[\"palestine_military\"] or l == examples[\"israel_military\"]:\n",
        "            labels[k] = 1\n",
        "        else:\n",
        "            labels[k] = 0\n",
        "    examples = tokenizer(examples[\"text\"])\n",
        "    examples[\"label\"] = labels\n",
        "    return examples"
      ],
      "metadata": {
        "id": "ucxG7gyQfS7Y"
      },
      "id": "ucxG7gyQfS7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00c5f19",
      "metadata": {
        "id": "d00c5f19"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "X_tr = []\n",
        "for i in range(len(X_train)):\n",
        "    X_tr.append(X_train[i][0])\n",
        "\n",
        "train_dict = {}\n",
        "train_dict[\"text\"] = X_tr\n",
        "train_dict[\"palestine_sympathy\"] = y_sympathy_p_train\n",
        "train_dict[\"israel_sympathy\"] = y_sympathy_i_train\n",
        "train_dict[\"palestine_military\"] = y_military_p_train\n",
        "train_dict[\"israel_military\"] = y_military_i_train\n",
        "train_ds = Dataset.from_dict(train_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8e5d75",
      "metadata": {
        "id": "9e8e5d75"
      },
      "outputs": [],
      "source": [
        "X_val = []\n",
        "for i in range(len(X_validation)):\n",
        "    X_val.append(X_validation[i][0])\n",
        "\n",
        "val_dict = {}\n",
        "val_dict[\"text\"] = X_val\n",
        "val_dict[\"palestine_sympathy\"] = y_sympathy_p_val\n",
        "val_dict[\"israel_sympathy\"] = y_sympathy_i_val\n",
        "val_dict[\"palestine_military\"] = y_military_p_val\n",
        "val_dict[\"israel_military\"] = y_military_i_val\n",
        "val_ds = Dataset.from_dict(val_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7a818d",
      "metadata": {
        "id": "8b7a818d"
      },
      "outputs": [],
      "source": [
        "X_tst = []\n",
        "for i in range(len(X_test)):\n",
        "    X_tst.append(X_test[i][0])\n",
        "\n",
        "test_dict = {}\n",
        "test_dict[\"text\"] = X_tst\n",
        "test_dict[\"palestine_sympathy\"] = y_sympathy_p_test\n",
        "test_dict[\"israel_sympathy\"] = y_sympathy_i_test\n",
        "test_dict[\"palestine_military\"] = y_military_p_test\n",
        "test_dict[\"israel_military\"] = y_military_i_test\n",
        "test_ds = Dataset.from_dict(test_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = {\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds}\n",
        "for dset in ds:\n",
        "    ds[dset] = ds[dset].map(preprocess_function, remove_columns=[\"text\", \"palestine_sympathy\", \"israel_sympathy\", \"palestine_military\", \"israel_military\"])"
      ],
      "metadata": {
        "id": "Sf6-Vt_QhMfJ"
      },
      "id": "Sf6-Vt_QhMfJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PS_LABELS = [\"PS0\", \"PS1\", \"PS2\"]\n",
        "PS_INDICES = range(3)\n",
        "IS_LABELS = [\"IS0\", \"IS1\", \"IS2\"]\n",
        "IS_INDICES = range(3,6)\n",
        "PM_LABELS = [\"PM0\", \"PM1\", \"PM2\"]\n",
        "PM_INDICES = range(6,9)\n",
        "IM_LABELS = [\"IM0\", \"IM1\", \"IM2\"]\n",
        "IM_INDICES = range(9,12)"
      ],
      "metadata": {
        "id": "O3Hu1yw4iFYV"
      },
      "id": "O3Hu1yw4iFYV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# credit: https://lajavaness.medium.com/multiclass-and-multilabel-text-classification-in-one-bert-model-95c54aab59dc\n",
        "# showed a way to find the max argument for individual slices of our labels\n",
        "def predict(logits):\n",
        "    ret = np.zeros(logits.shape)\n",
        "\n",
        "    PS_index = np.argmax(logits[:,PS_INDICES], axis=-1)\n",
        "    ret[range(len(ret)), PS_index] = 1\n",
        "    IS_index = np.argmax(logits[:,IS_INDICES], axis=-1) + 3\n",
        "    ret[range(len(ret)), IS_index] = 1\n",
        "    PM_index = np.argmax(logits[:,PM_INDICES], axis=-1) + 6\n",
        "    ret[range(len(ret)), PM_index] = 1\n",
        "    IM_index = np.argmax(logits[:, IM_INDICES], axis=-1) + 9\n",
        "    ret[range(len(ret)), IM_index] = 1\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "1MlV7ED4it2k"
      },
      "id": "1MlV7ED4it2k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# credit: https://lajavaness.medium.com/multiclass-and-multilabel-text-classification-in-one-bert-model-95c54aab59dc\n",
        "# custom compute metrics function to accomodate for the multiple classes\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    final_metrics = {}\n",
        "\n",
        "    # get predictions from logits\n",
        "    predictions = predict(logits)\n",
        "\n",
        "    # f1 scores per class\n",
        "    final_metrics[\"f1_micro_for_PS\"] = f1_score(labels[:, PS_INDICES], predictions[:, PS_INDICES], average=\"micro\")\n",
        "    final_metrics[\"f1_micro_for_IS\"] = f1_score(labels[:, IS_INDICES], predictions[:, IS_INDICES], average=\"micro\")\n",
        "    final_metrics[\"f1_micro_for_PM\"] = f1_score(labels[:, PM_INDICES], predictions[:, PM_INDICES], average=\"micro\")\n",
        "    final_metrics[\"f1_micro_for_IM\"] = f1_score(labels[:, IM_INDICES], predictions[:, IM_INDICES], average=\"micro\")\n",
        "    # global f1 scores\n",
        "    final_metrics[\"f1_micro\"] = f1_score(labels, predictions, average=\"micro\")\n",
        "    final_metrics[\"f1_macro\"] = f1_score(labels, predictions, average=\"macro\")\n",
        "    final_metrics[\"f1_weighted\"] = f1_score(labels, predictions, average=\"weighted\")\n",
        "\n",
        "    # classification reports\n",
        "    print(\"Classification report for PS: \")\n",
        "    print(classification_report(labels[:, PS_INDICES], predictions[:, PS_INDICES], zero_division=0))\n",
        "    print(\"Classification report for IS: \")\n",
        "    print(classification_report(labels[:, IS_INDICES], predictions[:, IS_INDICES], zero_division=0))\n",
        "    print(\"Classification report for PM: \")\n",
        "    print(classification_report(labels[:, PM_INDICES], predictions[:, PM_INDICES], zero_division=0))\n",
        "    print(\"Classification report for IM: \")\n",
        "    print(classification_report(labels[:, IM_INDICES], predictions[:, IM_INDICES], zero_division=0))\n",
        "    return final_metrics"
      ],
      "metadata": {
        "id": "3OY5RL-5k41Y"
      },
      "id": "3OY5RL-5k41Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pylab import plt\n",
        "\n",
        "# our own custom function to graph the training loss, validation loss, and weighted f1 score after each epoch\n",
        "def loss_graphing():\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    weighted_f1 = []\n",
        "    loss = trainer.state.log_history\n",
        "    for dic in loss:\n",
        "      if 'eval_loss' in dic:\n",
        "        validation_loss.append(dic['eval_loss'])\n",
        "        weighted_f1.append(dic['eval_f1_weighted'])\n",
        "      elif 'loss' in dic:\n",
        "        training_loss.append(dic['loss'])\n",
        "      else:\n",
        "        final_loss = dic\n",
        "\n",
        "    epochs_val = range(1,len(validation_loss)+1)\n",
        "    epochs_train = range(1,len(training_loss)+1)\n",
        "\n",
        "    plt.plot(epochs_train, training_loss, label='Training Loss')\n",
        "    plt.plot(epochs_val, validation_loss, label='Validation Loss')\n",
        "    plt.plot(epochs_val, weighted_f1, label='Weighted F1')\n",
        "    plt.axhline(y=0.6, color='r', linestyle='--', linewidth=2)\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(np.arange(1, 7, 1))"
      ],
      "metadata": {
        "id": "rY_8dKBJNChs"
      },
      "id": "rY_8dKBJNChs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# credit: https://lajavaness.medium.com/multiclass-and-multilabel-text-classification-in-one-bert-model-95c54aab59dc\n",
        "# custom loss function in order to take the imbalance of classes into account\n",
        "\n",
        "class MultiTaskClassificationTrainer(Trainer):\n",
        "    def __init__(self, group_weights=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.group_weights = group_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]\n",
        "\n",
        "        PS_inp = logits[:, 0:3].type(torch.float32)\n",
        "        PS_lab = labels[:, 0:3].type(torch.float32)\n",
        "        IS_inp = logits[:, 3:6].type(torch.float32)\n",
        "        IS_lab = labels[:, 3:6].type(torch.float32)\n",
        "        PM_inp = logits[:, 6:9].type(torch.float32)\n",
        "        PM_lab = labels[:, 6:9].type(torch.float32)\n",
        "        IM_inp = logits[:, 9:12].type(torch.float32)\n",
        "        IM_lab = labels[:, 9:12].type(torch.float32)\n",
        "\n",
        "        # our own custom weights - majority of the tweaking between training was done on these\n",
        "        wPS = torch.Tensor([7.435,1.77,3.17])\n",
        "        wIS = torch.Tensor([11.475,1.45,4.11])\n",
        "        wPM = torch.Tensor([2.36,1.87,12.41])\n",
        "        wIM = torch.Tensor([2.32,2.52,5.58])\n",
        "\n",
        "        PS_loss = torch.nn.functional.cross_entropy(PS_inp, PS_lab,weight=wPS)\n",
        "        IS_loss = torch.nn.functional.cross_entropy(IS_inp, IS_lab,weight=wIS)\n",
        "        PM_loss = torch.nn.functional.cross_entropy(PM_inp, PM_lab,weight=wPM)\n",
        "        IM_loss = torch.nn.functional.cross_entropy(IM_inp, IM_lab,weight=wIM)\n",
        "\n",
        "        loss = self.group_weights[0] * PS_loss + self.group_weights[1] * IS_loss + self.group_weights[2] * PM_loss + self.group_weights[3] * IM_loss\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "P3Awnd9wmRm7"
      },
      "id": "P3Awnd9wmRm7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9cd486",
      "metadata": {
        "id": "3d9cd486"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./content/oogabooga\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    load_best_model_at_end=False,\n",
        "    logging_strategy=\"epoch\",\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "metadata": {
        "id": "zlcyNnOrnH9E"
      },
      "id": "zlcyNnOrnH9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = MultiTaskClassificationTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    group_weights=(0.25, 0.25, 0.25, 0.25)\n",
        ")"
      ],
      "metadata": {
        "id": "Cl80dG38nkKC"
      },
      "id": "Cl80dG38nkKC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "M0PIPBxdoBkg"
      },
      "id": "M0PIPBxdoBkg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_graphing()"
      ],
      "metadata": {
        "id": "7Cc_BsVI9MRZ"
      },
      "id": "7Cc_BsVI9MRZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=ds[\"test\"])"
      ],
      "metadata": {
        "id": "Jje4kVJyoMiC"
      },
      "id": "Jje4kVJyoMiC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_graphing()"
      ],
      "metadata": {
        "id": "oc7u1lqLvdjp"
      },
      "id": "oc7u1lqLvdjp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/bees\")"
      ],
      "metadata": {
        "id": "iQsF7v8LvT_P"
      },
      "id": "iQsF7v8LvT_P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)"
      ],
      "metadata": {
        "id": "bZCQOIpc9K7e"
      },
      "id": "bZCQOIpc9K7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['-', \"n\", \"+\"] * 4\n",
        "\n",
        "def classification(text):\n",
        "  classified = pipe(text)\n",
        "  scores = []\n",
        "\n",
        "  for label in classified[0]:\n",
        "    scores.append(label[\"score\"])\n",
        "\n",
        "  scores = np.array(scores)\n",
        "  print(scores)\n",
        "  PS = np.argmax(scores[0:3])\n",
        "  IS = np.argmax(scores[3:6])\n",
        "  PM = np.argmax(scores[6:9])\n",
        "  IM = np.argmax(scores[9:12])\n",
        "\n",
        "  print(np.array([\"PS: \"+labels[PS], \"IS: \"+labels[IS], \"PM: \"+labels[PM], \"IM: \"+labels[IM]]).reshape(4,1))"
      ],
      "metadata": {
        "id": "LXlICSLC9keR"
      },
      "id": "LXlICSLC9keR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import sys\n",
        "import re\n",
        "\n",
        "#neither work w/ JS, old works only for a few sites\n",
        "def findingTextNewer(soup):\n",
        "  paragraphs = (soup.find_all(\"p\"))\n",
        "\n",
        "  for p in paragraphs:\n",
        "    depth = 0\n",
        "    for _ in p.parents:\n",
        "      depth = depth + 1\n",
        "\n",
        "    if str(depth) in depths:\n",
        "      depths[str(depth)] = depths[str(depth)] + len(p.text)\n",
        "    else:\n",
        "      depths[str(depth)] = 1\n",
        "\n",
        "  level = int(max(depths, key=lambda key: depths[key]))\n",
        "  # while(depths[str(level)] > 100):\n",
        "  #   del depths[str(level)]\n",
        "  #   level = int(max(depths, key=lambda key: depths[key]))\n",
        "\n",
        "  for p in paragraphs:\n",
        "    d = 0\n",
        "    for _ in p.parents:\n",
        "      d = d + 1\n",
        "\n",
        "    if (d == level):\n",
        "      INP = p.get_text().strip()\n",
        "      if INP != \"\":\n",
        "        print(INP)\n",
        "        classification(INP)\n",
        "\n",
        "#test websites:\n",
        "#CNN - https://www.cnn.com/2023/12/21/politics/trump-legal-chaos-2024/index.html\n",
        "#NYT - https://www.nytimes.com/2023/12/20/movies/home-alone-mccallisters-wealth.html\n",
        "#washington post - https://www.washingtonpost.com/world/2023/12/21/al-shifa-hospital-gaza-hamas-israel/\n",
        "#cnbc - https://www.cnbc.com/2023/12/21/former-trump-lawyer-rudy-giuliani-files-for-bankruptcy-protection.html\n",
        "#fox news - https://www.foxnews.com/politics/maine-forced-delay-vote-ev-mandate-amid-widespread-power-outages\n",
        "#bbc - https://www.bbc.com/news/world-middle-east-67764664\n",
        "#reuters - https://www.reuters.com/world/middle-east/egypt-seeks-broker-gaza-ceasefire-hamas-israel-assert-demands-2023-12-21/ - not working\n",
        "#al jazeera - https://www.aljazeera.com/news/2023/12/21/why-is-israels-military-killing-so-many-of-its-own\n",
        "#nbc - https://www.nbcnews.com/news/world/israels-offensive-hamas-gaza-succeeding-rcna130745\n",
        "#the guardian - https://www.theguardian.com/world/2023/dec/21/israel-idf-accused-targeting-journalists-gaza\n",
        "\n",
        "depths = {}\n",
        "\n",
        "headers = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\"}\n",
        "\n",
        "url = input(\"Enter the link to a news article: \")\n",
        "\n",
        "if matches := re.match(r\"^http?(s):\\/\\/(www\\.)?(.+)\\.co.+$\",url):\n",
        "  page = requests.get(url, headers=headers)\n",
        "  scraped = BeautifulSoup(page.content,\"html.parser\")\n",
        "else:\n",
        "  print(\"invald link\")\n",
        "\n",
        "#https://www.cnn.com/2023/12/21/politics/trump-legal-chaos-2024/index.html\n",
        "\n",
        "findingTextNewer(scraped)"
      ],
      "metadata": {
        "id": "9W4ig3r5w35e"
      },
      "id": "9W4ig3r5w35e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}